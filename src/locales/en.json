{
  "senior_advice": "Senior's Advice:",
  "numpy_module": {
    "title": "1. Memory Slicing Visualization (4x4 Matrix)",
    "view_label": "View (Reference)",
    "view_desc": "Basic Slicing always returns a view. Modifying it will change the original matrix memory directly!",
    "fancy_label": "Copy (New Memory)",
    "fancy_desc": "Fancy Indexing creates a copy. This involves memory allocation and is less efficient for large scale usage.",
    "mask_label": "Copy (New Memory)",
    "mask_desc": "Boolean Mask also triggers memory copy. It is a powerful tool for extracting specific conditional data.",
    "instruction": "Select a slicing method to observe NumPy's memory behavior...",
    "broadcast_title": "2. Broadcasting Mechanism",
    "broadcast_advice": "Broadcasting doesn't actually copy data in memory, but virtually maps addresses during iteration. Core rule: Compare from the last dimension forward, they must either be equal or one of them is <1>1</1>."
  },
  "backprop_module": {
    "title": "Chain Rule Tracker",
    "output_layer": "Output Layer",
    "error_val": "Error Value",
    "calculating": "Deriving Gradient...",
    "track_grad": "Track Gradient Δwjk",
    "notebook_title": "Derivation Notebook (COMP2211 Core)",
    "step1_title": "Step 1: Chain Rule Expansion",
    "step2_title": "Step 2: Define Error Signal",
    "step2_desc": "Let <1>\\delta_k = - \\frac{\\partial E}{\\partial net_k}</1>. Under MSE loss:",
    "step3_title": "Step 3: Weight Update Amount",
    "advice": "Remember, backpropagation is essentially a process of 'finding the culprit'. Each layer weight <1>w_{jk}</1> is asking: How much of the error <3>E</3> am I responsible for? <5>\\delta_k</5> is the 'blame letter' passed back from the previous layer."
  },
  "kernel_module": {
    "operation": "Operation:",
    "input_image": "Input Image",
    "kernel": "Kernel",
    "output_feature_map": "Output Feature Map",
    "advice": "A convolution kernel is essentially a local feature filter. When you use the <1>\\text{Laplacian}</1> operator, it is approximating the second derivative and can quickly capture 'sudden changes' in pixel brightness."
  },
  "bayes": {
    "basics": {
      "title": "1. Bayes Rule Core Analysis",
      "terms": {
        "posterior": "Posterior",
        "posterior_desc": "Update of belief <1>B</1> after evidence <3>E</3>.",
        "prior": "Prior",
        "prior_desc": "Probability of belief <1>B</1> before any evidence.",
        "likelihood": "Likelihood",
        "likelihood_desc": "Probability of evidence <1>E</1> given belief <3>B</3> is true.",
        "marginal": "Marginal",
        "marginal_desc": "Total probability of evidence <1>E</1> under all conditions."
      }
    },
    "fire_case": {
      "title": "2. Practice: Smoke & Fire Case (Notes P.15)",
      "p_fire": "Prob <1>P(\\text{Fire})</1>",
      "p_smoke": "Prob <1>P(\\text{Smoke})</1>",
      "p_smoke_given_fire": "Prob <1>P(\\text{Smoke}|\\text{Fire})</1>",
      "advice": "Classic case from notes: Fire is rare (Prior=1%), but smoke is common (Marginal=10%, maybe BBQ). Bayes tells us probability of fire given smoke is only <1>9\\%</1>. <3>Evidence ≠ Conclusion, Prior matters!</3>",
      "result_title": "Prob of Fire given Smoke",
      "calc_process": "Calculation:"
    },
    "naive": {
      "input_title": "Feature Input",
      "smoothing": "Smoothing <1>\\alpha</1>:",
      "log_mode": "LOG-SUM MODE",
      "product_mode": "PRODUCT MODE",
      "result_title": "Classification Result (Notes P.29)",
      "likelihood_chain": "Likelihood Chain",
      "prob_for": "Probabilities for {{cls}}",
      "advice": "Key point from Notes P.23: Bayes is 'Naive' because it assumes features are <1>conditionally independent</1>. Efficient for spam filtering. Watch out for <3>Zero Frequency Problem</3>, increase <5>\\alpha</5> to see changes!"
    }
  },
  "app": {
    "title": "CS/AI Interactive Learning Lab",
    "subtitle": "COMP2211 Machine Learning Notebook",
    "version": "v2.0 Katex Enhanced",
    "tabs": {
      "bayesBasics": "Bayes Basics",
      "naiveBayes": "Naive Bayes",
      "numpy": "NumPy Mechanism",
      "backprop": "Backpropagation",
      "kernel": "Kernel Lab"
    },
    "section": {
      "bayesBasics": {
        "title": "Bayes Theorem Basics",
        "subtitle": "From Math Definition to Intuitive Update"
      },
      "naiveBayes": {
        "title": "Naive Bayes Inference",
        "subtitle": "Classifier with Feature Conditional Independence"
      },
      "numpy": {
        "title": "NumPy Internal Mechanism",
        "subtitle": "Deep Dive into Memory Address Mapping & Broadcasting Logic"
      },
      "backprop": {
        "title": "Backpropagation & Gradient Update",
        "subtitle": "Chain Rule is the Bridge between Error and Weights"
      },
      "kernel": {
        "title": "Kernel Laboratory",
        "subtitle": "Real-time Simulation of Edge Feature Extraction"
      }
    },
    "sidebar": {
      "knowledge_core": "Knowledge Core",
      "exam_tip": {
        "title": "Exam Tip",
        "content": "Exam Note: <1>Broadcasting</1> can only extend dimensions of Size 1. <3>A \\cdot B</3> (np.dot) and <5>A \\odot B</5> (Element-wise) have completely different physical meanings!"
      }
    },
    "footer": {
      "copyright": "© 2026 KnowIt COMP2211 ML Learning Lab. Education through interaction.",
      "latency": "Latency: 12ms",
      "render": "Render: KaTeX High-Def"
    }
  }
}
